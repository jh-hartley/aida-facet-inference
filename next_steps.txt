•	Keep refining the prompts (huge amount of trial/error here)
- Finish the documenrtations
- Clean up logging
•	Clean up the full-dataset inference script (functional, but written fast with ChatGPT, so ugly and not maintainable)
•	Introduce a self-review loop (where the model critiques/refines its own outputs)
•	Build a lightweight way to retrieve richer web scrape context for better citation, probably through tooling.
To elaborate, the self-review loop would involve a second LLM pass, which would then critique and either validate or request a revision. This cycle would continue until the result meets a confidence threshold or the loop cap is reached. It’s a common pattern in agent-based systems for tasks like data validation, reasoning chains, and summarisation QA. Architecturally, this is where tools like LangGraph or AutoGen could be useful and simple conversation memory (literally just prepending the chat history to each llm call) can improve results. We’ve only used LangChain so far, but those libraries are well-suited to managing branching logic and memory across iterations.
I also want to clarify that the richer web scrape referenced would use LangChain’s web scraping tooling. I don’t currently have access to the data/capability obviously, but from Matt’s description, it sounds like it’s easy to dump raw HTML into the context window (and we can fit in hundreds of thousands more words and still remain cheaper than humans). We could handle this either by (1) giving the LLM access to a scrape trigger tool that re-prompts it in a loop with new HTML context, or (2) adding a flag to the output to mark products for re-inference once scraped data becomes available. The latter is probably more practical to avoid delays in the main loop. I am tempted to rule out the EAN lookup approach because it’s tough to catch when the information is wrong. Maybe the LLM would catch the inaccuracy and predict low confidence but I really have no idea if that would work regularly.



Matt is one of my clients for context. It might be good to break this into subtasks.